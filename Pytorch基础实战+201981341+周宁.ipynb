{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd344de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf4102e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -1.5846e+29,  0.0000e+00],\n",
      "        [-1.5846e+29,  2.3168e+04,  4.5908e-41]])\n",
      "tensor([3., 4., 5., 7.])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor(2,3)\n",
    "b = torch.FloatTensor([3,4,5,7])\n",
    "print(a)\n",
    "print(b)\n",
    "print(type(a))\n",
    "print(b.dtype)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a451c46",
   "metadata": {},
   "source": [
    "float32\\float64(double已经被新版本取消了）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93fc39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.int32)\n",
      "tensor([3, 4, 5, 7], dtype=torch.int32)\n",
      "<class 'torch.Tensor'>\n",
      "torch.int32\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.IntTensor(2,3)\n",
    "b = torch.IntTensor([3,4,5,7])\n",
    "print(a)\n",
    "print(b)\n",
    "print(type(a))\n",
    "print(b.dtype)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97993c88",
   "metadata": {},
   "source": [
    "这个是生成int型tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f47239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8521, 0.5316, 0.0618],\n",
      "        [0.6139, 0.0399, 0.7142]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,3)\n",
    "print(a)\n",
    "print(type(a))\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319367ad",
   "metadata": {},
   "source": [
    "这个部分是服从0-1的均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1f06be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8018, -0.5918,  0.0641],\n",
      "        [ 0.5202,  0.3213,  0.2316]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "print(type(a))\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167e110",
   "metadata": {},
   "source": [
    " 随机生成的浮点数的取值满 均值为0、方差为1的正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b867ab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7302,  0.1805,  0.2081],\n",
      "        [ 0.2082,  1.4065,  0.2813]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "print(type(a))\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8420a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "tensor([1., 2., 3., 4., 5.])\n",
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "tensor([ 1.6848, -1.4714,  9.5572,  1.3403,  4.6049])\n"
     ]
    }
   ],
   "source": [
    "#mean为张  std为张 \n",
    "mean = torch.arange(1,6,dtype=torch.float)\n",
    "std = torch.arange(1,6,dtype=torch.float)\n",
    "t = torch.normal(mean,std)\n",
    "print(mean)\n",
    "print(type(mean))\n",
    "print(mean.dtype)\n",
    "print(std)\n",
    "print(type(std))\n",
    "print(std.dtype)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe900f4",
   "metadata": {},
   "source": [
    "mean和std是张量，生成规则是mean1，std1的正态分布采样；mean2，std2 balabala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e16bede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1511,  0.2069, -1.7728, -1.5113, -0.2910])\n"
     ]
    }
   ],
   "source": [
    "#mean为标  std为标 \n",
    "t = torch.normal(0.2,1.0,size=(5,))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed529ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:2,std:tensor([1., 2., 3.])\n",
      "tensor([ 2.0666, -1.9278, -1.2204])\n"
     ]
    }
   ],
   "source": [
    "#mean为标  std为张 \n",
    "mean = 2\n",
    "std = torch.arange(1,4,dtype=torch.float)\n",
    "t = torch.normal(mean,std)\n",
    "print(\"mean:{},std:{}\".format(mean,std))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b0c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:tensor([1., 2., 3.]),std:2\n",
      "tensor([0.5247, 0.8697, 5.0507])\n"
     ]
    }
   ],
   "source": [
    "#mean为张  std为标 \n",
    "mean = torch.arange(1,4,dtype=torch.float)\n",
    "std = 2\n",
    "t = torch.normal(mean,std)\n",
    "print(\"mean:{},std:{}\".format(mean,std))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d05701f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n",
      "torch.float32\n",
      "tensor([1, 2, 3, 4])\n",
      "torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-59e56a4df551>:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  a = torch.range(1,5,1)\n"
     ]
    }
   ],
   "source": [
    "a = torch.range(1,5,1)\n",
    "b = torch.arange(1,5,1)\n",
    "print(a)\n",
    "print(a.dtype)\n",
    "print(b)\n",
    "print(b.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60f43d",
   "metadata": {},
   "source": [
    "range好像不让用了，生成的type也不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6d2d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[ 1.0675,  3.4921, -2.3565, -0.1579, -0.4538],\n",
      "        [-1.2986, -0.3241, -1.7721, -0.8157, -1.0444]])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(2,5)\n",
    "b=torch.randn_like(a)\n",
    "c=torch.zeros(2,5)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e4b87",
   "metadata": {},
   "source": [
    "# 总结\n",
    "前面这段学了pytorch的生成机制\n",
    "+ 生成机制\n",
    "    - 可以通过FloatTensor（2,3）生成指定形状，数据类型，但是数据随机的张量\n",
    "    - 可以通过FloatTensor（【2,3,4，5】）生成指定内容的张量\n",
    "    - 可以通过rand和randn（2,5）生成形状指定，数据服从均匀、正态分布的张量\n",
    "    - 可以通过range（1,5,1）生成指定步长和起始终止的float一维张量（包含5）\n",
    "    - 可以通过arange（1,5,1）生成int张量（不包含5）\n",
    "    - 可以通过ones/zeros生成指定形状的1,0矩阵\n",
    "    - 可以通过randn_like生成和某张量大小类似的张量（逻辑上好看些）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce2ca1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0090, -0.3117, -0.3332],\n",
      "        [ 0.7601, -0.4121,  0.6323]])\n",
      "tensor([[1.0090, 0.3117, 0.3332],\n",
      "        [0.7601, 0.4121, 0.6323]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.abs(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c092339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4166, -1.1137, -0.1583],\n",
      "        [-1.6606, -0.2080, -0.9639]])\n",
      "tensor([[-1.6722,  0.0184, -0.8000],\n",
      "        [-0.9226, -0.7697, -1.2405]])\n",
      "tensor([[-1.2556, -1.0953, -0.9583],\n",
      "        [-2.5832, -0.9777, -2.2044]])\n",
      "tensor([[-1.2556, -1.0953, -0.9583],\n",
      "        [-2.5832, -0.9777, -2.2044]])\n",
      "a.add后的a值  tensor([[ 0.4166, -1.1137, -0.1583],\n",
      "        [-1.6606, -0.2080, -0.9639]])\n",
      "tensor([[-1.2556, -1.0953, -0.9583],\n",
      "        [-2.5832, -0.9777, -2.2044]])\n",
      "a.add_后的a值  tensor([[-1.2556, -1.0953, -0.9583],\n",
      "        [-2.5832, -0.9777, -2.2044]])\n",
      "tensor([[-0.2627,  0.5623, -0.3668],\n",
      "        [ 0.9965,  1.6342, -1.3331]])\n",
      "tensor([[ 9.7373, 10.5623,  9.6332],\n",
      "        [10.9965, 11.6342,  8.6669]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.randn(2,3)\n",
    "print(b)\n",
    "c = torch.add(a,b)\n",
    "print(c)\n",
    "print(a.add(b))\n",
    "print(\"a.add后的a值 \",a)\n",
    "print(a.add_(b))\n",
    "print(\"a.add_后的a值 \",a)\n",
    "d = torch.randn(2,3)\n",
    "print(d)\n",
    "e = torch.add(d,10)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d5188",
   "metadata": {},
   "source": [
    "所有对于原张量进行改动的都要加上_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60a5d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5743,  0.6774,  1.0702],\n",
      "        [ 1.2786, -1.6769,  0.6661]])\n",
      "tensor([[-0.1000,  0.1000,  0.1000],\n",
      "        [ 0.1000, -0.1000,  0.1000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.clamp(a, -0.1, 0.1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ec0d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2776, -0.6916,  2.4188],\n",
      "        [ 0.8735, -1.3093, -2.1834]])\n",
      "tensor([[-0.5290,  0.2988,  1.2027],\n",
      "        [-0.2817, -1.0404,  0.5863]])\n",
      "tensor([[ 2.4153, -2.3149,  2.0111],\n",
      "        [-3.1015,  1.2585, -3.7239]])\n",
      "tensor([[-1.3122, -1.1534,  0.6019],\n",
      "        [-1.1550, -0.6156,  0.2867]])\n",
      "tensor([[-0.1312, -0.1153,  0.0602],\n",
      "        [-0.1155, -0.0616,  0.0287]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.randn(2,3)\n",
    "print(b)\n",
    "c = torch.div(a,b)\n",
    "print(c)\n",
    "d = torch.randn(2,3)\n",
    "print(d)\n",
    "e = torch.div(d, 10)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90a2fb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6783, 0.5456, 0.7342],\n",
      "        [0.7695, 0.7927, 0.3905]])\n",
      "tensor([[0.7687, 0.6713, 0.9451],\n",
      "        [0.8055, 0.2721, 0.0520]])\n",
      "tensor([[0.5215, 0.3662, 0.6939],\n",
      "        [0.6198, 0.2157, 0.0203]])\n",
      "tensor([[ 1.8437, -0.2861, -0.3771],\n",
      "        [-0.3717, -0.0838, -0.0642]])\n",
      "tensor([[18.4369, -2.8614, -3.7713],\n",
      "        [-3.7166, -0.8381, -0.6418]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,3)\n",
    "print(a)\n",
    "b = torch.rand(2,3)\n",
    "print(b)\n",
    "c = torch.mul(a,b)\n",
    "print(c)\n",
    "d = torch.randn(2,3)\n",
    "print(d)\n",
    "e = torch.mul(d, 10)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd85e672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0916, -1.9133,  0.8955],\n",
      "        [-0.2680, -0.3965,  0.2904]])\n",
      "tensor([[0.0084, 3.6608, 0.8020],\n",
      "        [0.0718, 0.1572, 0.0843]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.pow(a,2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66ba9c",
   "metadata": {},
   "source": [
    "注意mul和div、pow都是对位运算，不是矩阵运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "744124a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3416,  0.2638,  0.7572],\n",
      "        [-0.2134, -1.2006, -0.3186]])\n",
      "tensor([[-0.8635,  1.2733],\n",
      "        [-0.5113,  0.1311],\n",
      "        [ 0.3946,  1.5734]])\n",
      "tensor([[-0.1311,  1.6610],\n",
      "        [ 0.6725, -0.9304]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.randn(3,2)\n",
    "print(b)\n",
    "c = torch.mm(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751de10",
   "metadata": {},
   "source": [
    "这个才是矩阵运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc64f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2984, -1.2511,  1.1866],\n",
      "        [-0.9507,  0.9329, -0.0231]])\n",
      "tensor([-0.8250,  0.8170,  0.4355])\n",
      "tensor([-1.5766,  1.5365])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.randn(3)\n",
    "print(b)\n",
    "c = torch.mv(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca418e59",
   "metadata": {},
   "source": [
    "这个是矩阵和向量乘，matrix vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b933f4",
   "metadata": {},
   "source": [
    "# 总结\n",
    "这一部分讲了矩阵的运算\n",
    "+ 乘法总结\n",
    "    - abs(a)用来搞绝对值\n",
    "    - b = torch.clamp(a, -0.1, 0.1)用来剪切，可以用来实现relu\n",
    "    - c = torch.div(a,b)对位相乘\n",
    "    - c = torch.div(a,b)对位相除\n",
    "    - b = torch.pow(a,2)每个平方\n",
    "    - c = torch.mm(a,b)矩阵乘法\n",
    "    - c = torch.mv(a,b)矩阵乘向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aea9f120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor = torch.rand(3,4)\n",
    "a_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3489234e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8573c565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53bc2116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8398, 0.8531, 0.4312, 0.9303, 0.0077, 0.7480],\n",
       "        [0.4492, 0.1241, 0.1130, 0.4367, 0.3984, 0.8872]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor.reshape(2,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f500cef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 1, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_data=torch.ones(1,2,3,1,5)\n",
    "multi_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25ba4274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_data_temp=torch.squeeze(multi_data)\n",
    "multi_data_temp.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4f1ca",
   "metadata": {},
   "source": [
    "上面这个地方没太搞懂，再来测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "828f1a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=torch.ones(1,2)\n",
    "m=torch.ones(2,2)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d59a9e",
   "metadata": {},
   "source": [
    "看懂了，所以上面那个（1，2，3，1，5）是一个五维的张量，规定了每一维的数据\n",
    "那么squeeze就是从所有维度中剔除size 1的维度。\n",
    "这个squeeze会直接改变张量的大小，内容，不用像运算里面那样加上_才可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1af8b09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_data_temp = torch.squeeze(multi_data,0)\n",
    "multi_data_temp.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd32bbe",
   "metadata": {},
   "source": [
    "他有一个dim参数，可以指定移出哪一个维度，如果size不为1那么不发生改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef8fd871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_data = torch.ones(3,4)\n",
    "multi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "163a6212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_data_temp = torch.unsqueeze(multi_data,1)\n",
    "multi_data_temp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "767b48da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_data_temp = torch.unsqueeze(multi_data,-1)\n",
    "multi_data_temp.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a4fb8",
   "metadata": {},
   "source": [
    "有了squeeze当然也可以unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bac8314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor = torch.arange(4).reshape(1,4)\n",
    "a_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40953a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d84b2c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_tensor=a_tensor.expand(3,4)\n",
    "b_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e72c34",
   "metadata": {},
   "source": [
    "    size是有()的方法，shape是tensor的属性不用加（）\n",
    "    expand在维数为1的维度上 行拓展 其他维度保持与原来一样即可,可以先使用.size()查看原始维度 如果没有维数为1的维度则不 拓展 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f77830d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [0, 1, 2, 3]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f644c418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor=a_tensor.reshape(4,1)\n",
    "a_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0ef0786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [3, 3]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_tensor=a_tensor.expand(4,2)\n",
    "b_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ce68d",
   "metadata": {},
   "source": [
    "torch.repeat()  根据指定的维度进行填充 指定的值为扩展的倍数 指定的维数不少于原始维数 但可以增加维数以增加新维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "784fc404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "tensor([[0, 1, 0, 1],\n",
      "        [2, 3, 2, 3]])\n",
      "tensor([[0, 1, 0, 1],\n",
      "        [2, 3, 2, 3],\n",
      "        [0, 1, 0, 1],\n",
      "        [2, 3, 2, 3]])\n",
      "tensor([[[0, 1, 0, 1, 0, 1],\n",
      "         [2, 3, 2, 3, 2, 3],\n",
      "         [0, 1, 0, 1, 0, 1],\n",
      "         [2, 3, 2, 3, 2, 3]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(4).reshape(2,2)\n",
    "print(a)\n",
    "b = a.repeat(1,2)\n",
    "print(b)\n",
    "c = a.repeat(2,2)\n",
    "print(c)\n",
    "d = a.repeat(1,2,3)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76623281",
   "metadata": {},
   "source": [
    "torch.cat()  将所给tensor按照指定维度拼接。这个dim还是对于维度进行指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab57c5fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9891, 0.3842, 0.6609],\n",
      "        [0.2209, 0.5084, 0.5876]])\n",
      "tensor([[0.9891, 0.3842, 0.6609],\n",
      "        [0.2209, 0.5084, 0.5876],\n",
      "        [0.9891, 0.3842, 0.6609],\n",
      "        [0.2209, 0.5084, 0.5876]])\n",
      "tensor([[0.9891, 0.3842, 0.6609, 0.9891, 0.3842, 0.6609],\n",
      "        [0.2209, 0.5084, 0.5876, 0.2209, 0.5084, 0.5876]])\n"
     ]
    }
   ],
   "source": [
    "a_tensor = torch.rand(2,3)\n",
    "print(a_tensor)\n",
    "b_tensor = torch.cat((a_tensor,a_tensor), dim = 0)\n",
    "print(b_tensor)\n",
    "c_tensor = torch.cat((a_tensor,a_tensor), dim = 1)\n",
    "print(c_tensor)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40d457a5",
   "metadata": {},
   "source": [
    "torch.stack()  将所给tensor按照指定的新维度上拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b06894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1962, 0.6870, 0.2754],\n",
      "        [0.6375, 0.4121, 0.1425]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[[0.1962, 0.6870, 0.2754],\n",
      "         [0.6375, 0.4121, 0.1425]],\n",
      "\n",
      "        [[0.1962, 0.6870, 0.2754],\n",
      "         [0.6375, 0.4121, 0.1425]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.1962, 0.6870, 0.2754],\n",
      "         [0.1962, 0.6870, 0.2754]],\n",
      "\n",
      "        [[0.6375, 0.4121, 0.1425],\n",
      "         [0.6375, 0.4121, 0.1425]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.1962, 0.1962],\n",
      "         [0.6870, 0.6870],\n",
      "         [0.2754, 0.2754]],\n",
      "\n",
      "        [[0.6375, 0.6375],\n",
      "         [0.4121, 0.4121],\n",
      "         [0.1425, 0.1425]]])\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "a_tensor = torch.rand(2,3)\n",
    "print(a_tensor)\n",
    "print(a_tensor.shape)\n",
    "b_tensor = torch.stack((a_tensor,a_tensor), dim = 0)\n",
    "print(b_tensor)\n",
    "print(b_tensor.shape)\n",
    "c_tensor = torch.stack((a_tensor,a_tensor), dim = 1)\n",
    "print(c_tensor)\n",
    "print(c_tensor.shape)\n",
    "d_tensor = torch.stack((a_tensor,a_tensor), dim = 2)\n",
    "print(d_tensor)\n",
    "print(d_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93096255",
   "metadata": {},
   "source": [
    "    这个拼接的过程：https://blog.csdn.net/qq_39709535/article/details/80803003?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164739634716781685322584%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164739634716781685322584&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-80803003.142^v2^control,143^v4^control&utm_term=torch.cat&spm=1018.2226.3001.4187\n",
    "    \n",
    "    torch.column_stack 同torch.hstack 、torch.row_stack （同torch.vstack） 可分别对列和行角度拼接 是一般拼接操作的细化\n",
    "    \n",
    "    能够进行平移当然也能进行分割，用torch.chunk进行处理，返回的是一个张量元祖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "938713e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6626, 0.0549, 0.1099, 0.7250],\n",
      "        [0.9665, 0.8746, 0.2710, 0.5915],\n",
      "        [0.1601, 0.9390, 0.9620, 0.5720],\n",
      "        [0.8252, 0.3096, 0.0801, 0.8363]])\n",
      "(tensor([[0.6626, 0.0549, 0.1099, 0.7250],\n",
      "        [0.9665, 0.8746, 0.2710, 0.5915]]), tensor([[0.1601, 0.9390, 0.9620, 0.5720],\n",
      "        [0.8252, 0.3096, 0.0801, 0.8363]]))\n",
      "(tensor([[0.6626, 0.0549],\n",
      "        [0.9665, 0.8746],\n",
      "        [0.1601, 0.9390],\n",
      "        [0.8252, 0.3096]]), tensor([[0.1099, 0.7250],\n",
      "        [0.2710, 0.5915],\n",
      "        [0.9620, 0.5720],\n",
      "        [0.0801, 0.8363]]))\n"
     ]
    }
   ],
   "source": [
    "a_tensor = torch.rand(4,4)\n",
    "print(a_tensor)\n",
    "b_tensor = torch.chunk(a_tensor, 2, dim=0)\n",
    "print(b_tensor)\n",
    "c_tensor = torch.chunk(a_tensor, 2, dim=1)\n",
    "print(c_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb1e9c",
   "metadata": {},
   "source": [
    "torch.split()  将tensor按指定维度划分 可通过list指定划分的块大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e7986b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7181, 0.9808, 0.2986, 0.4026],\n",
      "        [0.2820, 0.6069, 0.0490, 0.3525],\n",
      "        [0.6960, 0.0921, 0.9289, 0.8230],\n",
      "        [0.7128, 0.5931, 0.2625, 0.6398]])\n",
      "(tensor([[0.7181, 0.9808, 0.2986, 0.4026],\n",
      "        [0.2820, 0.6069, 0.0490, 0.3525]]), tensor([[0.6960, 0.0921, 0.9289, 0.8230],\n",
      "        [0.7128, 0.5931, 0.2625, 0.6398]]))\n",
      "(tensor([[0.7181, 0.9808, 0.2986, 0.4026],\n",
      "        [0.2820, 0.6069, 0.0490, 0.3525],\n",
      "        [0.6960, 0.0921, 0.9289, 0.8230]]), tensor([[0.7128, 0.5931, 0.2625, 0.6398]]))\n",
      "(tensor([[0.7181],\n",
      "        [0.2820],\n",
      "        [0.6960],\n",
      "        [0.7128]]), tensor([[0.9808, 0.2986, 0.4026],\n",
      "        [0.6069, 0.0490, 0.3525],\n",
      "        [0.0921, 0.9289, 0.8230],\n",
      "        [0.5931, 0.2625, 0.6398]]))\n"
     ]
    }
   ],
   "source": [
    "a_tensor = torch.rand(4,4)\n",
    "print(a_tensor)\n",
    "b_tensor = torch.split(a_tensor, 2, dim=0)\n",
    "print(b_tensor)\n",
    "c_tensor = torch.split(a_tensor, [3,1], dim=0)\n",
    "print(c_tensor)\n",
    "d_tensor = torch.split(a_tensor, [1,3], dim=1)\n",
    "print(d_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77101e3c",
   "metadata": {},
   "source": [
    "# 总结\n",
    "这一部分讲拼接和分割，还是挺好玩的。但是要对dim和分割的方法非常熟悉，各个维度怎么操作的要清晰。\n",
    "+ 拼接分割\n",
    "     - squeeze干掉维度1的\n",
    "     - unsqueeze在指定维度插入size1的新维度的新张量\n",
    "     - expand拓展维度size1的\n",
    "     - repeat填充，参数是行重复次数，列重复次数，新维度重复次数\n",
    "     - cat拼接\n",
    "     - stack在新维度上进行拼接\n",
    "     - chunk分割成不同张量\n",
    "     - split不仅可以分割还可以指定分割大小\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bee5449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a_tensor = torch.ones(5)\n",
    "b_np = a_tensor.numpy()\n",
    "print(type(a_tensor),type(b_np))\n",
    "print(a_tensor, b_np)\n",
    "a_tensor += 1\n",
    "print(a_tensor, b_np)\n",
    "b_np += 1\n",
    "print(a_tensor, b_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25957a82",
   "metadata": {},
   "source": [
    "说明通过numpy生成的是会跟随一起改变的，他们共享相同的内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be9071a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.])\n",
      "[4. 4. 4. 4. 4.] tensor([4., 4., 4., 4., 4.])\n",
      "[5. 5. 5. 5. 5.] tensor([5., 5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "c_tensor = torch.from_numpy(b_np)\n",
    "print(type(c_tensor),type(b_np))\n",
    "print(b_np, c_tensor)\n",
    "b_np += 1\n",
    "print(b_np, c_tensor)\n",
    "c_tensor += 1\n",
    "print(b_np, c_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61555d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a_np = np.ones(5)\n",
    "b_tensor = torch.from_numpy(a_np)\n",
    "print(a_np, b_tensor)\n",
    "a_np += 1\n",
    "print(a_np, b_tensor)\n",
    "b_tensor += 1\n",
    "print(a_np, b_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aca8f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a_np = np.ones(5)\n",
    "c_tensor = torch.tensor(a_np)\n",
    "a_np += 1\n",
    "print(a_np, c_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ca366",
   "metadata": {},
   "source": [
    "# 总结\n",
    "pytorch的tensor和numpy共用一个内存，转换很快。但如果使用torch.tensor将numpy数组转换成tensor，该方法会进行数据拷贝，返回的tensor和原来数据不共享内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac604c",
   "metadata": {},
   "source": [
    "好了，你已经学会pytorch了，下面开始科研吧（bushi）先搭建一个简单的神经网络试试看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06bc3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15cad0",
   "metadata": {},
   "source": [
    "单层神经网络，一批次输入100个数据，input_data每个数据包含特征1000，hidden-layer定义经过隐藏层后保留的数据特征个数。output_layer输出的数据，最后需要10分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc99c00",
   "metadata": {},
   "source": [
    "一个批次的数据从输入到输出的完整过程是：先输入100个具有1000个特征的数据 经隐藏层后变成100个具\n",
    "有100个特征的数据 再经过输出层后输出100个具有10个分类结果值的数据 在得到输出结果之后计算损失并\n",
    "进行后向传播 这样一次模型的训练就完成了 然后循环整个流程就可以完成指定次数的训练 并达到优化模型\n",
    "参数的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f1794e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1000])\n",
      "torch.Size([1000, 100])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "#准备数据\n",
    "x = torch.randn(batch_n, input_data)\n",
    "y = torch.randn(batch_n, output_data)\n",
    "#权 初始化\n",
    "w1 = torch.randn(input_data, hidden_layer)\n",
    "w2 = torch.randn(hidden_layer, output_data)\n",
    "print(x.shape)\n",
    "print(w1.shape)\n",
    "print(w2.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "372966f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定后向传播的次数和梯度下降使用的学习 率。\n",
    "epoch_n = 20\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c89fc114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:50054160.0000\n",
      "Epoch:1, Loss:93008848.0000\n",
      "Epoch:2, Loss:268537920.0000\n",
      "Epoch:3, Loss:485145216.0000\n",
      "Epoch:4, Loss:134842192.0000\n",
      "Epoch:5, Loss:8219215.0000\n",
      "Epoch:6, Loss:3935161.5000\n",
      "Epoch:7, Loss:2775647.0000\n",
      "Epoch:8, Loss:2329662.7500\n",
      "Epoch:9, Loss:2077342.0000\n",
      "Epoch:10, Loss:1891058.7500\n",
      "Epoch:11, Loss:1736490.1250\n",
      "Epoch:12, Loss:1602641.2500\n",
      "Epoch:13, Loss:1484484.6250\n",
      "Epoch:14, Loss:1379354.3750\n",
      "Epoch:15, Loss:1285317.6250\n",
      "Epoch:16, Loss:1201085.2500\n",
      "Epoch:17, Loss:1125148.1250\n",
      "Epoch:18, Loss:1056340.1250\n",
      "Epoch:19, Loss:993898.6250\n"
     ]
    }
   ],
   "source": [
    "# 练\n",
    "for epoch in range(epoch_n):\n",
    "    #前向传播\n",
    "    h1 = x.mm(w1) #[100,100]\n",
    "    h1 = h1.clamp(min = 0) #使用clamp方法 行 剪 将小于 的值全  新赋值为0  就像加上了一个ReL\n",
    "    y_pred = h1.mm(w2) #[100,10]\n",
    "    # 算损失\n",
    "    loss = (y_pred - y ).pow(2).sum() # 差值的 算使用了均方 差函数\n",
    "    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch, loss))\n",
    "    #后向传播  算梯度使用的是每个 点的 式求导结果\n",
    "    grad_y_pred = 2*(y_pred - y)\n",
    "    grad_w2 = h1.t().mm(grad_y_pred)\n",
    "    grad_h = grad_y_pred.clone()\n",
    "    grad_h = grad_h.mm(w2.t())\n",
    "    grad_h.clamp_(min=0)\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    #权 参数优化\n",
    "    w1 -= learning_rate*grad_w1\n",
    "    w2 -= learning_rate*grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3741b3",
   "metadata": {},
   "source": [
    "    通常来说我们是不用手写梯度的，上面这个作为人工智能专业的学生最好也掌握一下。\n",
    "    接下来为大家介绍自动梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "563fa298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "\n",
    "#准备数据\n",
    "x = Variable(torch.randn(batch_n, input_data), requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n, output_data), requires_grad = False)\n",
    "\n",
    "#权 初始化\n",
    "w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad = True)\n",
    "w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6b1dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:39241920.0000\n",
      "Epoch:1, Loss:54877476.0000\n",
      "Epoch:2, Loss:159202256.0000\n",
      "Epoch:3, Loss:420514112.0000\n",
      "Epoch:4, Loss:331000384.0000\n",
      "Epoch:5, Loss:3672128.7500\n",
      "Epoch:6, Loss:2522745.2500\n",
      "Epoch:7, Loss:1980010.5000\n",
      "Epoch:8, Loss:1662198.7500\n",
      "Epoch:9, Loss:1441561.0000\n",
      "Epoch:10, Loss:1271295.0000\n",
      "Epoch:11, Loss:1132102.3750\n",
      "Epoch:12, Loss:1014682.3750\n",
      "Epoch:13, Loss:914313.1875\n",
      "Epoch:14, Loss:827432.8125\n",
      "Epoch:15, Loss:751806.3750\n",
      "Epoch:16, Loss:685570.8750\n",
      "Epoch:17, Loss:627122.2500\n",
      "Epoch:18, Loss:575456.8125\n",
      "Epoch:19, Loss:529569.8125\n"
     ]
    }
   ],
   "source": [
    "# 定后向传播的次数和梯度下降使用的学习 率。\n",
    "epoch_n = 20\n",
    "learning_rate = 1e-6\n",
    "# 练\n",
    "for epoch in range(epoch_n):\n",
    "    #前向传播\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    # 算损失\n",
    "    loss = (y_pred - y ).pow(2).sum() # 差值的 算使用了均方 差函数\n",
    "    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch, loss))\n",
    "    #后向传播 \n",
    "    loss.backward()\n",
    "    #权 参数更新\n",
    "    w1.data -= learning_rate*w1.grad.data\n",
    "    w2.data -= learning_rate*w2.grad.data\n",
    "    w1.grad.data.zero_() #梯度值置 \n",
    "    w2.grad.data.zero_() #梯度值置 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "289d4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e261962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def _init_(self):\n",
    "        super(Model, self)._init_()\n",
    "    def forward(self, input, w1, w2):\n",
    "        x = torch.mm(input, w1)\n",
    "        x = torch.clamp(x, min = 0)\n",
    "        x = torch.mm(x, w2)\n",
    "        return x\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5344ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dbbe3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:44922680.0000\n",
      "Epoch:1, Loss:101032960.0000\n",
      "Epoch:2, Loss:420813856.0000\n",
      "Epoch:3, Loss:860729728.0000\n",
      "Epoch:4, Loss:45626224.0000\n",
      "Epoch:5, Loss:19612918.0000\n",
      "Epoch:6, Loss:12024367.0000\n",
      "Epoch:7, Loss:8339947.0000\n",
      "Epoch:8, Loss:6195304.0000\n",
      "Epoch:9, Loss:4809432.0000\n",
      "Epoch:10, Loss:3851065.7500\n",
      "Epoch:11, Loss:3156098.7500\n",
      "Epoch:12, Loss:2633715.5000\n",
      "Epoch:13, Loss:2230605.7500\n",
      "Epoch:14, Loss:1912554.6250\n",
      "Epoch:15, Loss:1657291.3750\n",
      "Epoch:16, Loss:1449427.6250\n",
      "Epoch:17, Loss:1278160.7500\n",
      "Epoch:18, Loss:1135629.3750\n",
      "Epoch:19, Loss:1015743.5625\n"
     ]
    }
   ],
   "source": [
    "#准备数据\n",
    "x = Variable(torch.randn(batch_n, input_data), requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n, output_data), requires_grad = False)\n",
    "#权 初始化\n",
    "w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad = True)\n",
    "w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad = True)\n",
    "# 定后向传播的次数和梯度下降使用的学习 率。\n",
    "epoch_n = 20\n",
    "learning_rate = 1e-6\n",
    "# 练\n",
    "for epoch in range(epoch_n):\n",
    "    #前向传播\n",
    "    y_pred = model(x, w1, w2)\n",
    "    # 算损失\n",
    "    loss = (y_pred - y ).pow(2).sum() # 差值的 算使用了均方 差函数\n",
    "    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch, loss))\n",
    "    #后向传播 \n",
    "    loss.backward()\n",
    "    #权 参数更新\n",
    "    w1.data -= learning_rate*w1.grad.data\n",
    "    w2.data -= learning_rate*w2.grad.data\n",
    "   \n",
    "    w1.grad.data.zero_() #梯度值置 \n",
    "    w2.grad.data.zero_() #梯度值置 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353eb777",
   "metadata": {},
   "source": [
    "### 通过上面这个过程我们就可以自己指定传播函数啦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed49a1f",
   "metadata": {},
   "source": [
    "# 模型搭建参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b450ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "#准备数据\n",
    "x = Variable(torch.randn(batch_n, input_data), requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n, output_data), requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a1f9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.nn.Sequential(\n",
    "torch.nn.Linear(input_data, hidden_layer),\n",
    "torch.nn.ReLU(),\n",
    "torch.nn.Linear(hidden_layer, output_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f54138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "models = torch.nn.Sequential(\n",
    "torch.nn.Linear(input_data, hidden_layer),\n",
    "torch.nn.ReLU(),\n",
    "torch.nn.Linear(hidden_layer, output_data)\n",
    ")\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa2f51d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (Line1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (Relu1): ReLU()\n",
      "  (Line2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "from collections import OrderedDict\n",
    "models = torch.nn.Sequential(OrderedDict([\n",
    "(\"Line1\", torch.nn.Linear(input_data, hidden_layer)),\n",
    "(\"Relu1\", torch.nn.ReLU()),\n",
    "(\"Line2\", torch.nn.Linear(hidden_layer, output_data))])\n",
    ")\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72c37a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定后向传播的次数和梯度下降使用的学习 率。\n",
    "epoch_n = 10000\n",
    "learning_rate = 1e-4\n",
    "#定义损失函数\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b68ca4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6667)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "loss_f = torch.nn.MSELoss()\n",
    "# x = Variable(torch.randn(100,100))\n",
    "# y = Variable(torch.randn(100,100))\n",
    "x = Variable(torch.FloatTensor([1,2,3]))\n",
    "y = Variable(torch.FloatTensor([0,0,0]))\n",
    "loss = loss_f(x, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "641af1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3333)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "loss_f = torch.nn.L1Loss()\n",
    "# x = Variable(torch.randn(100,100))\n",
    "# y = Variable(torch.randn(100,100))\n",
    "x = Variable(torch.FloatTensor([1,2,4]))\n",
    "y = Variable(torch.FloatTensor([0,0,0]))\n",
    "loss = loss_f(x, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57984c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0129,  1.4924,  0.8029,  0.5426,  1.1644],\n",
      "        [ 1.4464, -0.0689,  0.9223,  0.7673,  0.0966],\n",
      "        [-0.4218, -0.5972, -0.4448, -0.1068, -0.5999]])\n",
      "tensor([1, 3, 0])\n",
      "tensor(1.4270)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "loss_f = torch.nn.CrossEntropyLoss()\n",
    "x = Variable(torch.randn(3,5))\n",
    "print(x)\n",
    "y = Variable(torch.LongTensor(3).random_(5))\n",
    "print(y)\n",
    "# x = Variable(torch.FloatTensor([[1,2],[3,2]]))\n",
    "# y = Variable(torch.LongTensor([1,0]))\n",
    "loss = loss_f(x, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f65b673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:1.0405\n",
      "Epoch:1000, Loss:0.9616\n",
      "Epoch:2000, Loss:0.8929\n",
      "Epoch:3000, Loss:0.8323\n",
      "Epoch:4000, Loss:0.7778\n",
      "Epoch:5000, Loss:0.7282\n",
      "Epoch:6000, Loss:0.6829\n",
      "Epoch:7000, Loss:0.6412\n",
      "Epoch:8000, Loss:0.6024\n",
      "Epoch:9000, Loss:0.5660\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "#准备数据\n",
    "x = Variable(torch.randn(batch_n, input_data), requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n, output_data), requires_grad = False)\n",
    "#定义模型\n",
    "models = torch.nn.Sequential(\n",
    "torch.nn.Linear(input_data, hidden_layer),\n",
    "torch.nn.ReLU(),\n",
    "torch.nn.Linear(hidden_layer, output_data)\n",
    ")\n",
    "# 定后向传播的次数和梯度下降使用的学习 率。\n",
    "epoch_n = 10000\n",
    "learning_rate = 1e-4\n",
    "#定义损失函数\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# 练\n",
    "for epoch in range(epoch_n):\n",
    "    #前向传播\n",
    "    y_pred = models(x)\n",
    "    \n",
    "    # 算损失\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if epoch%1000 == 0:\n",
    "        print(\"Epoch:{}, Loss:{:.4f}\".format(epoch, loss))\n",
    "    models.zero_grad()\n",
    "    #后向传播 \n",
    "    loss.backward()\n",
    "    #权 参数更新\n",
    "    for param in models.parameters():\n",
    "        param.data -= param.grad.data*learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51b9d5",
   "metadata": {},
   "source": [
    "    和tensorflow基本一致的操作方式，问题不大。分析一下他的Module类就可以啦\n",
    "    \n",
    "    最后还可以使用torch.optim进一步简化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302124bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:1.0532\n",
      "Epoch:1, Loss:1.0313\n",
      "Epoch:2, Loss:1.0099\n",
      "Epoch:3, Loss:0.9891\n",
      "Epoch:4, Loss:0.9687\n",
      "Epoch:5, Loss:0.9489\n",
      "Epoch:6, Loss:0.9294\n",
      "Epoch:7, Loss:0.9104\n",
      "Epoch:8, Loss:0.8919\n",
      "Epoch:9, Loss:0.8737\n",
      "Epoch:10, Loss:0.8561\n",
      "Epoch:11, Loss:0.8389\n",
      "Epoch:12, Loss:0.8221\n",
      "Epoch:13, Loss:0.8058\n",
      "Epoch:14, Loss:0.7898\n",
      "Epoch:15, Loss:0.7741\n",
      "Epoch:16, Loss:0.7588\n",
      "Epoch:17, Loss:0.7439\n",
      "Epoch:18, Loss:0.7293\n",
      "Epoch:19, Loss:0.7151\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "#准备数据\n",
    "x = Variable(torch.randn(batch_n, input_data), requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n, output_data), requires_grad = False)\n",
    "#定义模型\n",
    "models = torch.nn.Sequential(\n",
    "torch.nn.Linear(input_data, hidden_layer),\n",
    "torch.nn.ReLU(),\n",
    "torch.nn.Linear(hidden_layer, output_data)\n",
    ")\n",
    "# 定后向传播的次数和梯度下降使用的学习 率。\n",
    "epoch_n = 20\n",
    "learning_rate = 1e-4\n",
    "#定义损失函数\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "#定义优化方法\n",
    "optimzer = torch.optim.Adam(models.parameters(), lr = learning_rate)\n",
    "# 练\n",
    "for epoch in range(epoch_n):\n",
    "    #前向传播\n",
    "    y_pred = models(x)\n",
    "    # 算损失\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch, loss))\n",
    "    optimzer.zero_grad()\n",
    "    #后向传播 \n",
    "    loss.backward()\n",
    "    optimzer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d0ab0",
   "metadata": {},
   "source": [
    "试试看增加一下markdown line并提交到github中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
